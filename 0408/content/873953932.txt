Big datais a term bandied around the technology sector with great ease, but little precision. While exact definitions are hard to come by, what is clear is that the ability to gather, process and store huge amounts of data is unparalleled. The sheer quantity of information and the speed at which it is consumed means conventional methods of handling it are no longer sufficient.According to analysts IDC, the amount of digital information created and replicated rose by 62% in 2010 to nearly 800,000 petabytes, which, IDC says, would fill a stack of DVDs reaching from the earth to the moon and back. By 2020, that pile of DVDs would stretchhalfway to Mars.Nick Halstead, CEO of DataSift, a company that specializes in handling massive data, says the New York Stock Exchange produces one terabyte of data a day; Facebook generates in excess of 20 terabytes every day; while the CERN laboratory in Geneva produces more than 40 terabytes a day, or roughly twice the entire text content of the U.S. Library of Congress every day.In its recent report, "Big data: The next frontier for innovation, competition and productivity", McKinsey Global Institute estimated that a retailer usingbig datacould increase its operating margin by more than 60%. The U.S. could reduce its healthcare expenditure by 8% and government administrators in Europe could save more than [euro]100 billion ($143 billion).The definition MGI uses forbig datais deliberately vague and not based on a specific number.It refers instead to sets of data which are too large for current conventional database tools to capture, store, manage and analyze. It also takes account of the differences between sectors in the type of data and software available.The ability to process this vast flow of data in real time will become a business imperative. In a traditional environment, information is gathered, put in a database, which is stored on a disc, and then indexed. Queries are then run against the database. But traditional databases are simply not up to the task of storing and handling the sheer quantity of data. It requires new types of databases able to span tens, hundreds, even thousands of servers. Yahoo, for example, runs a database cluster that spans 40,000 servers.Advanced processing power is necessary in an environment where decisions are made in nanoseconds. At the very least, databases have to be stored in memory, demanding massive increases in server power and storage.At its most extreme is a process called "complex event processing", which instead uses the flow of raw data and matches the query against it, looking for patterns. Typical uses of CEP include high-frequency financial trading, but other examples include sending a game player a special offer at exactly the right moment in a game, persuading them to make an in-game purchase.This is a point picked up by Kristian Segerstrale, co-founder of social games company Playfish, who says: "Getting the information is important, but even more important will be the ability to react in real time to data and structure experiments to learn empirically from user behavior."Ilja Laurs, the founder and chief executive of GetJar, the world's largest open mobile application store, says there is a point where, "the level of sophistication and optimization of various business processes starts to exceed a human's capacity to understand them".Data MiningHe says: "Today, algorithms, used for positioning products in supermarkets to presenting a dynamic price grid when you book a flight ticket to maximize flight occupancy, are becoming critical to every business. To achieve the next step in efficiency it's necessary to build data mining, machine learning, optimization algorithms for every business."Some remain skeptical about the current claims being made forbig data. They point out that not all data is created equal and that, according to IDC, the fastest growing type of data is unstructured in the form of e-mails, instant messages and other "human-friendly data".This may not be appropriate for some of the much-vaunted new tools for analyzingbig data, such as in-memory database technology. This is extremely fast because it does not have to read and write from a disc: "But if you have a paragraph of an email and you put that into an in-memory database it has no more idea of what it means than an in-disc database," says Mike Lynch, chief executive of Autonomy. "If, in a few years time, they do manage to develop tools for analysis of human-friendly data, it will offer great insights into the way organizations are run, but we're not there yet," he says.Credit: By Nick Clayton